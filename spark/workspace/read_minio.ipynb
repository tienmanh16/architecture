{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aed2c96e-4e91-453a-a8e6-d42aec0fbf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, explode\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bccd8d6a-cbfa-42a0-87f3-f15db79bba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Minio\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31ff2899-c982-4635-9471-4ceca93c99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"datalake\"\n",
    "json_file_path = \"raw_data/*/*\"\n",
    "\n",
    "# Construct the S3 URI\n",
    "s3_uri = f\"s3a://{bucket_name}/{json_file_path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3a997e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------+--------+--------------------+----------------+---------------+\n",
      "|             current|               daily|              hourly|    lat|     lon|            minutely|        timezone|timezone_offset|\n",
      "+--------------------+--------------------+--------------------+-------+--------+--------------------+----------------+---------------+\n",
      "|{100, 301.34, 172...|[{98, 297.36, 172...|[{100, 300.68, 17...|21.0278|105.8342|[{1722937620, 0.5...|    Asia/Bangkok|          25200|\n",
      "|{40, 297.9, 17229...|[{34, 296.77, 172...|[{52, 297.84, 172...|10.8231|106.6297|[{1722937620, 0.0...|Asia/Ho_Chi_Minh|          25200|\n",
      "|{75, 299.1, 17229...|[{25, 296.56, 172...|[{74, 298.79, 172...|10.0333|105.7667|[{1722937620, 0.0...|Asia/Ho_Chi_Minh|          25200|\n",
      "|{20, 298.94, 1722...|[{51, 295.15, 172...|[{28, 298.55, 172...|16.0675|108.2203|[{1722937620, 0.0...|Asia/Ho_Chi_Minh|          25200|\n",
      "+--------------------+--------------------+--------------------+-------+--------+--------------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON file\n",
    "df = spark.read.json(s3_uri)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16141445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0630dfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------------+------+---------+----------+----------+--------+--------+----------+----------+------+----+----------+--------+---------+----------+----------------+----+---+------+\n",
      "|    lat|     lon|        timezone|clouds|dew_point|        dt|feels_like|humidity|pressure|   sunrise|    sunset|  temp| uvi|visibility|wind_deg|wind_gust|wind_speed|     description|icon| id|  main|\n",
      "+-------+--------+----------------+------+---------+----------+----------+--------+--------+----------+----------+------+----+----------+--------+---------+----------+----------------+----+---+------+\n",
      "|21.0278|105.8342|    Asia/Bangkok|   100|   301.34|1722937597|    315.14|      68|    1002|1722897143|1722943963|308.14|0.57|     10000|     123|     3.91|      2.22|      light rain| 10d|500|  Rain|\n",
      "|10.8231|106.6297|Asia/Ho_Chi_Minh|    40|    297.9|1722937597|    313.16|      62|    1006|1722897763|1722942962|306.16|0.37|     10000|     241|     8.05|      5.36|scattered clouds| 03d|802|Clouds|\n",
      "|10.0333|105.7667|Asia/Ho_Chi_Minh|    75|    299.1|1722937598|    304.31|      94|    1006|1722898029|1722943109|300.15|0.62|     10000|     260|     null|      3.09|   broken clouds| 04d|803|Clouds|\n",
      "|16.0675|108.2203|Asia/Ho_Chi_Minh|    20|   298.94|1722937598|    313.14|      66|    1005|1722896976|1722942985|306.14|0.58|     10000|     120|     null|      4.12|      few clouds| 02d|801|Clouds|\n",
      "+-------+--------+----------------+------+---------+----------+----------+--------+--------+----------+----------+------+----+----------+--------+---------+----------+----------------+----+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flattened_df = df.select(\n",
    "    col(\"lat\"),\n",
    "    col(\"lon\"),\n",
    "    col(\"timezone\"),\n",
    "    col(\"current.*\"),\n",
    "    col(\"current.weather\").getItem(0).alias(\"current_weather\")\n",
    ")\n",
    "\n",
    "# Select all fields from the current struct and flatten the first element of the weather array\n",
    "flattened_df = flattened_df.select(\n",
    "    col(\"lat\"),\n",
    "    col(\"lon\"),\n",
    "    col(\"timezone\"),\n",
    "    col(\"clouds\"),\n",
    "    col(\"dew_point\"),\n",
    "    col(\"dt\"),\n",
    "    col(\"feels_like\"),\n",
    "    col(\"humidity\"),\n",
    "    col(\"pressure\"),\n",
    "    col(\"sunrise\"),\n",
    "    col(\"sunset\"),\n",
    "    col(\"temp\"),\n",
    "    col(\"uvi\"),\n",
    "    col(\"visibility\"),\n",
    "    col(\"wind_deg\"),\n",
    "    col(\"wind_gust\"),\n",
    "    col(\"wind_speed\"),\n",
    "    col(\"current_weather.*\")\n",
    ")\n",
    "\n",
    "# Show the flattened DataFrame\n",
    "flattened_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f101c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06756e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jdbc:oracle:thin:@host.docker.internal:1521/xepdb1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle_url = f'jdbc:oracle:thin:@{os.getenv(\"DB_HOST\")}:{os.getenv(\"DB_PORT\")}/{os.getenv(\"DB_SERVICE\")}'\n",
    "oracle_properties = {\n",
    "    \"user\": os.getenv(\"DB_NAME\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\"),\n",
    "    \"driver\": \"oracle.jdbc.driver.OracleDriver\"\n",
    "}\n",
    "oracle_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "750b3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write the DataFrame to the Oracle table\n",
    "flattened_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", oracle_url) \\\n",
    "    .option(\"dbtable\", \"weather_data\") \\\n",
    "    .option(\"user\", oracle_properties[\"user\"]) \\\n",
    "    .option(\"password\", oracle_properties[\"password\"]) \\\n",
    "    .option(\"driver\", oracle_properties[\"driver\"]) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
